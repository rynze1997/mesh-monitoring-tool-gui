import pandas as pd
import string
from datetime import datetime, timedelta
import time


class MDR:
    
    def __init__ (self):
        # self.connections = [('E8:61:33:BD:38:1E', 'D3:D2:3E:4E:8D:93')] DD:77:D7:12:90:4E
        self.connections = [('E3:CA:10:B4:88:7A', 'E6:58:D8:31:2E:52')] 
        self.mac_label_map = {'D4:1B:20:25:CF:1D': {'number': 1, 'title': 'Black2'}, 'E6:58:D8:31:2E:52': {'number': 2, 'title': 'White'}, 'E3:CA:10:B4:88:7A': {'number': 3, 'title': 'Yellow'}, 'FD:31:20:A8:6C:10': {'number': 4, 'title': '3'}, 'DF:DC:E7:66:CC:01': {'number': 5, 'title': 'Black'}, 'E6:11:85:7C:B5:94': {'number': 6, 'title': '2'}, 'C3:32:96:81:99:D4': {'number': 7, 'title': 'Yellow2'}}
        self.node_neighbor_map = {1: [3, 5, 6, 2, 7, 4], 3: [1, 5, 4, 7, 6], 5: [1, 4, 7, 3, 2, 6], 2: [4, 7, 5, 6, 1], 4: [2, 5, 7, 3, 6, 1], 7: [5, 2, 4, 3, 6, 1], 6: [1, 2, 7, 5, 4, 3]}
    def data_processing_mdr(self):
        time_start = time.time()

        # Load your dataset
        df = pd.read_csv('.results/mdr.csv')
        df['Timestamp'] = df['Timestamp'].apply(self.get_packet_timestamp)
        
        
        result = []
        for source, destination in self.connections:
            result.append(self.calculate_mdr(df, source, destination, 215 ,217, self.node_neighbor_map))
            
        print("Time taken:", time.time() - time_start)
        
        return result 
    
    @staticmethod
    def get_first_occurrences(df, mac_address):
        """
        Find which packets are generated by the given MAC address.
        """

        # Group the DataFrame by 'Version', 'Index', and 'Payload' and get the first occurrence in each group
        copy_df = df.copy()
        copy_df['Payload'] = copy_df['Payload'].fillna('NoPayload')
        copy_df = copy_df.groupby(['Version', 'Index']).first().reset_index()
        
        # Filter the DataFrame to include only rows with the given MAC address
        copy_df = copy_df[copy_df['MAC'] == mac_address]

        # Convert the DataFrame to a list of Series and return it
        return copy_df
    
    
    def calculate_mdr(self, df, source_mac, destination_mac, source_index, destination_index, node_neighbor_map):
        
        '''
        Done in two steps:
        # STEP 1 # 
            1. Find all messages that originates from the source
            2. Separate all SET, GET messages on destination Index
            3. Destination node will answer to SET and GET messages with incremented version. Pair all SET and GET packets with all observed acknowledgements.
            The acknowledgement does not have to come from the destination node itself. The tool can miss it. But if the destination node has received 
            the SET or GET message and acknowledged it (and the tool missed it), the acknowledgement message will be rebroadcasted by all other nodes.
            4. If a pair is found to the source message, then there is an acknowledgement. Otherwise not.
            
        # STEP 2 #
            1. Separate all messages that the source sent on its own index and that are not SET or GET.
            2. Find all messages sent by the destination node.
            3. Pair all source node messages with destination node messages. If a pair is found, then there is a direct acknowledgement.
               If not, then the destination node has not rebroadcasted the message or the tool has not heard it.
               A node does not rebroadcast a message if it does not receive it or if the same message is rebroadcasted by other neighbors at least 4 times 
               before the destination node itself should rebroadcast. This is due to Trickle algorithm redundancy constant.
               If the destination node has not rebroadcasted the source message and neighbors have rebroadcasted it at least 4 times, then it can be said
               with high probability that the destination node has received it but dropped it due to trickle redundancy.
            4. Isolate all unacknowledged messages
            5. Find all neighbors of the destination node
            6. Find all packets that the neighbors has sent
            7. From all neighbor packets, get only those that are the same as the unacknowledged messages
            8. Apply a 32ms (First trickle period) window to all neighbors packets from previous step and count how many times each packet is rebroadcasted
            9. If any of those packets are rebroadcasted at least 4 times in any of the 32ms window, it can be considered that the destination node has received that 
               message and dropped its rebroadcast.
        '''
        
        # Find all new version messages that originates from the source
        source_messages = self.get_first_occurrences(df, source_mac)
        source_messages.to_csv(f'source_packets.csv', index=False)
        
        throughput_df = source_messages['Timestamp']
        througput_df = throughput_df.sort_values()
        # Find newest and oldes value
        newest_packet = througput_df.iloc[0]
        print(newest_packet)
        oldest_packet = througput_df.iloc[-1]
        print(oldest_packet)
        time_diff = (oldest_packet - newest_packet).total_seconds()
        print(time_diff)
        throughput = len(source_messages) / time_diff
        print(f"Average Throughput: {throughput} packets/s")
        
        
        ## STEP 1 ##
        # Isolate all SET and GET messages that originates from the source node
        source_set_and_get_messages = source_messages[((source_messages['Flags'] == '[SET]') | (source_messages['Flags'] == '[GET]')) & (source_messages['Index'] == destination_index)]
        source_set_and_get_messages.to_csv(f'set_and_get_packets.csv', index=False)
        
        # The destination must answer with new version messages if it receives a SET or a GET message from the source
        # To each SET and GET message that originates from the source, add a new column with the next version that corresponds to the version 
        # that the destination node must answer with
        source_set_and_get_messages = source_set_and_get_messages.copy()
        source_set_and_get_messages.loc[:, 'Next_Version'] = source_set_and_get_messages['Version'] + 1
        source_set_and_get_messages.to_csv(f'set_and_get_packets_next_version.csv', index=False)
        
        # Isolate all RESP and ACK from the dataframe before merging
        all_ack_and_resp_packets = df[((df['Flags'] == '[ACK]') | (df['Flags'] == '[RESP]'))]
        
        # If the destination node has received the SET or GET message by the source node, we can either catch the answer from the destination node itself
        # or from other nodes that has received the answer of the destination node and started re-broadcasting.
        # For this reason, we need to pair all SET and GET messages with all observed acknowledgements.
        merged_set_and_get_packets = pd.merge(source_set_and_get_messages, all_ack_and_resp_packets, 
                                    left_on=['Index', 'Next_Version'], 
                                    right_on=['Index', 'Version'], how='left', indicator=True)  # indicator adds _merge column
        # If a source message is rebroadcasted many times by many nodes it will be merged with all of them. We need only one of them to
        # understand if the destination node has received the message or not. So, we need to drop duplicates based on 'Index' and 'Version'
        merged_set_and_get_messages = merged_set_and_get_packets.drop_duplicates(subset=['Index', 'Version_x'])
        merged_set_and_get_messages.to_csv(f'merged_set_and_get_packets.csv', index=False)
        
        # From the merge, find all acknowledged packets and unacknowledged packets
        acknowledged_merge_set_and_get_messages = merged_set_and_get_messages[merged_set_and_get_messages['_merge'] == 'both']
        acknowledged_merge_set_and_get_messages.to_csv(f'acknowledged_merge_set_and_get_packets.csv', index=False)
        
        unacknowledged_merge_set_and_get_messages = merged_set_and_get_messages[merged_set_and_get_messages['_merge'] == 'left_only']
        unacknowledged_merge_set_and_get_messages.to_csv(f'unacknowledged_merge_set_and_get_packets.csv', index=False)
        # Rename columns for easier manipulation
        unacknowledged_merge_set_and_get_messages = unacknowledged_merge_set_and_get_messages.copy()
        unacknowledged_merge_set_and_get_messages.rename(columns={'Payload_x': 'Payload', 'Version_x': 'Version'}, inplace=True)
        
        ## STEP 2 ## Other messages
        # Isolate all other messages that originates from the source. Why?
        other_messages = source_messages[(source_messages['Flags'] != '[SET]') & (source_messages['Flags'] != '[GET]') & (source_messages['Index'] == source_index)]
        other_messages.to_csv(f'other_packets.csv', index=False)
        
        df.to_csv(f'df.csv', index=False)
        # Find all messages that are sent from destination
        destination_messages = df[df['MAC'] == destination_mac]
        destination_messages.to_csv(f'{destination_mac}_packets.csv', index=False)
        destination_messages= destination_messages.drop_duplicates(subset=['Version', 'Payload', 'Index'])
        destination_messages.to_csv(f'{destination_mac}.csv', index=False)
        
        # Pair all other messages with all observed acknowledgements from the destination node
        merged_other_messages = pd.merge(other_messages, destination_messages, 
                                    left_on=['Index', 'Version', 'Payload'], 
                                    right_on=['Index', 'Version', 'Payload'], how='left', indicator=True)
        merged_other_messages.to_csv(f'merged_other_packets.csv', index=False)
        # Find all acknowledged packets and unacknowledged packets
        acknowledged_other_messages = merged_other_messages[merged_other_messages['_merge'] == 'both']
        acknowledged_other_messages.to_csv(f'acknowledged_other_packets.csv', index=False)
        unacknowledged_other_messages = merged_other_messages[merged_other_messages['_merge'] == 'left_only']
        unacknowledged_other_messages.to_csv(f'unacknowledged_other_packets.csv', index=False)
        
        acks =  (len(acknowledged_merge_set_and_get_messages) + len(acknowledged_other_messages))
        total_messages = len(source_messages)
        mdr = acks / total_messages
        
        # STEP 2 #
        
        # Find all destination node neighbors
        neighbors = node_neighbor_map[self.mac_label_map[destination_mac]['number']]
        
        # If there is no neighbors then there is nothing else to do. Return the mdr
        if not neighbors:
            return mdr
        # Get MAC addresses of all neighbors
        neighbor_macs = [mac for mac, info in self.mac_label_map.items() if info['number'] in neighbors]
        
        # Get packets that belong to destination node neighbors
        neighbor_packets = df[df['MAC'].isin(neighbor_macs)]
        neighbor_packets = neighbor_packets.copy()
        neighbor_packets['Payload'] = neighbor_packets['Payload'].fillna('NoPayload')
        neighbor_packets.to_csv(f'neighbor_packets.csv', index=False)
        
        # From all neighbor packets, get only those that are the same as the unacknowledged messages
        criteria = unacknowledged_other_messages[['Version', 'Index', 'Payload']]
        neighbor_packets = neighbor_packets.merge(criteria, on=['Version', 'Index', 'Payload'], how='inner')
        
        # Apply rolling window of 32ms to all neighbor packets with unique version and index, and count how many times each packet is rebroadcasted
        neighbor_packets.reset_index(inplace=True)
        grouped_packets = neighbor_packets.groupby(['Index', 'Version'])
        rolling_window_result = grouped_packets.apply(lambda group: group.rolling(window=pd.Timedelta(32, 'ms'), on='Timestamp').count(), include_groups=False)
        rolling_window_result.to_csv(f'rolling_window_result.csv')
        
        # Get the maximum number of times each packet is rebroadcasted
        grouped_packets = rolling_window_result.groupby(['Index', 'Version'])
        max_rebroadcasts_per_group = pd.DataFrame(grouped_packets.apply(lambda group: group['index'].max()), columns=['Max'])
        max_rebroadcasts_per_group.to_csv(f'max_rebroadcasts_per_group.csv')
        indirect_acks = max_rebroadcasts_per_group[max_rebroadcasts_per_group['Max'] > 4]
        
        acks += len(indirect_acks)
        mdr = acks / total_messages
        
        print(f"Total amount of messages: {total_messages}")
        print(f"Acks: {acks}")
        print(f"MDR is: {mdr}")
        
    
    @staticmethod
    def filter_packets_by_mac(df, mac_address):
        """
        Filter packets in the dataframe by MAC address, ensuring unique versions and payloads
        """
        return df[df['MAC'] == mac_address].drop_duplicates(subset=['Version', 'Payload', 'Index'])
    
    def find_acknowledgements(self, df, source_packets, destination_packets):
        """
        For each original source packet, check if any of destination packets has same version and payload
        """
        acknowledged_packets = 0
        for source_packet in source_packets:
            matching_packets = destination_packets[(destination_packets['Version'] == source_packet['Version']) & 
                                            (destination_packets['Payload'] == source_packet['Payload']) &
                                            (destination_packets['Index'] == source_packet['Index'])]
            if not matching_packets.empty:
                acknowledged_packets += 1
            elif matching_packets.empty:
                # print(source_packet)
                if self.check_trickle_redundancy_constant(df, source_packet):
                    acknowledged_packets += 1
                
        return acknowledged_packets
    
    def check_trickle_redundancy_constant(self, df, source_packet):
        # Ensure source_packet is a Series or a dictionary
        source_packet_timestamp = self.get_packet_timestamp(source_packet)
        
        filtered_df = df[(df['Version'] == source_packet['Version']) & (df['Payload'] == source_packet['Payload'])]
        if filtered_df.empty:
            return False
        
        eligible_packets = []

        # Only eligible packets are those who are within 32 ms of the source packet
        for _, item in filtered_df.iterrows():
            packet_timestamp = self.get_packet_timestamp(item)
            if packet_timestamp - source_packet_timestamp < 32_000:
                eligible_packets.append(item)

        # Convert eligible_packets to a DataFrame correctly
        df_eligible = pd.DataFrame(eligible_packets)

        return (len(df_eligible) - 1) >= 4 ## Minus source packet
    
    @staticmethod
    def get_packet_timestamp(packet_timestamp: str):
        # Remove brackets and split by the dot
        parts = packet_timestamp.strip('[]').split('.')
        if len(parts) != 4:
            print(f"Invalid timestamp format: {packet_timestamp}")
            return None
        
        try:
            # Extract each part of the timestamp
            minutes, seconds, milliseconds, microseconds = [int(part) for part in parts]
            
            # Construct a timedelta since the minimum component we have is minutes
            # This timedelta represents the duration since the start of the hour
            delta = timedelta(minutes=minutes, seconds=seconds, milliseconds=milliseconds, microseconds=microseconds)
            
            # Construct a base datetime object (you might adjust this according to your needs)
            # Here we use a base date of 1st January 1970 with the hour set to 0
            base_datetime = datetime(1970, 1, 1, hour=0)
            
            # Add the timedelta to the base datetime
            final_datetime = base_datetime + delta
            return final_datetime
        except ValueError as e:
            print(f"Error parsing timestamp: {e}")
            return None
        
mdr = MDR()

mdr.data_processing_mdr()